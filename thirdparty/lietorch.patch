diff --git a/CMakeLists.txt b/CMakeLists.txt
new file mode 100644
index 0000000..4db02a0
--- /dev/null
+++ b/CMakeLists.txt
@@ -0,0 +1,232 @@
+cmake_minimum_required(VERSION 3.10)
+project(lietorch LANGUAGES CXX CUDA)
+
+set(CPP_STANDARD_VERSION "17" CACHE STRING "Desired C++ standard version") # We need C++17 since NVCC does not support C++20
+
+
+# Let's detect which NVCC standards are supported
+# Run "nvcc -h | grep -- '--std'" inside a Bash shell
+execute_process(
+    COMMAND bash -c "nvcc -h | grep -- '--std'"
+    OUTPUT_VARIABLE NVCC_OUTPUT
+    ERROR_VARIABLE NVCC_ERROR
+    RESULT_VARIABLE NVCC_RESULT
+    OUTPUT_STRIP_TRAILING_WHITESPACE
+)
+if(NVCC_RESULT)
+    message(WARNING "Failed to get NVCC supported standards: ${NVCC_ERROR}")
+    set(NVCC_OUTPUT "")
+endif()
+message(STATUS "Filtered NVCC output:\n${NVCC_OUTPUT}")
+
+# Detect GCC version
+execute_process(
+    COMMAND ${CMAKE_CXX_COMPILER} -dumpversion
+    OUTPUT_VARIABLE GCC_VERSION
+    OUTPUT_STRIP_TRAILING_WHITESPACE
+)
+string(REPLACE "." ";" VERSION_LIST ${GCC_VERSION})
+list(GET VERSION_LIST 0 GCC_MAJOR_VERSION)
+message(STATUS "Detected GCC version: ${GCC_VERSION}")
+
+# Check available C++ standards
+if(NVCC_OUTPUT MATCHES "c\\+\\+20" AND GCC_MAJOR_VERSION GREATER_EQUAL 10)
+    set(CPP_STANDARD_VERSION "20")
+elseif(NVCC_OUTPUT MATCHES "c\\+\\+17")
+    set(CPP_STANDARD_VERSION "17")
+elseif(NVCC_OUTPUT MATCHES "c\\+\\+14")
+    set(CPP_STANDARD_VERSION "14")
+else()
+    message(WARNING "No valid C++ standard found, defaulting to C++${CPP_STANDARD_VERSION}")
+endif()
+
+message(STATUS "Using C++ standard: C++${CPP_STANDARD_VERSION}")
+
+# Set default build type to Release
+# if(NOT CMAKE_BUILD_TYPE)
+#   set(CMAKE_BUILD_TYPE Release)
+# endif()
+
+# Set CMake policies
+# cmake_policy(SET CMP0148 NEW)
+# cmake_policy(SET CMP0146 NEW)
+
+# Set the C++ standard
+set(CMAKE_CXX_STANDARD ${CPP_STANDARD_VERSION})
+set(CMAKE_CXX_STANDARD_REQUIRED ON)
+
+# Generate compile_commands.json for tooling
+set(CMAKE_EXPORT_COMPILE_COMMANDS ON)
+
+# Set basic compiler flags
+set(CMAKE_C_FLAGS "-O2")
+set(CMAKE_CXX_FLAGS "-O2")
+
+# Function to detect CUDA architectures
+function(detect_cuda_architectures ARCHS)
+  execute_process(
+    COMMAND nvcc --list-gpu-arch
+    OUTPUT_VARIABLE GPU_ARCHS_OUTPUT
+    OUTPUT_STRIP_TRAILING_WHITESPACE
+    ERROR_QUIET
+  )
+
+  # Parse the output and extract architectures
+  string(REPLACE "\n" ";" GPU_ARCHS_LIST "${GPU_ARCHS_OUTPUT}")
+  set(DETECTED_ARCHS "")
+  foreach(ARCH ${GPU_ARCHS_LIST})
+    string(REGEX MATCH "compute_([0-9]+)" _ ${ARCH})
+    if(NOT "${CMAKE_MATCH_1}" STREQUAL "")
+      list(APPEND DETECTED_ARCHS "${CMAKE_MATCH_1}")
+    endif()
+  endforeach()
+
+  if(DETECTED_ARCHS)
+    set(${ARCHS} ${DETECTED_ARCHS} PARENT_SCOPE)
+  else()
+    message(WARNING "No CUDA architectures detected. Falling back to default.")
+    set(${ARCHS} "70" PARENT_SCOPE) # Default to a commonly supported architecture
+  endif()
+endfunction()
+
+# Detect CUDA architectures and set them
+detect_cuda_architectures(CUDA_ARCHITECTURES)
+
+# Set CUDA architectures
+set(CMAKE_CUDA_ARCHITECTURES ${CUDA_ARCHITECTURES})
+message(STATUS "Detected CUDA architectures: ${CMAKE_CUDA_ARCHITECTURES}")
+
+# Use the newer CUDA toolkit package
+find_package(CUDAToolkit QUIET)
+if (NOT CUDAToolkit_FOUND)
+  find_package(CUDA REQUIRED)
+endif()
+
+# Find Python (and its development files)
+find_package(Python3 REQUIRED COMPONENTS Interpreter Development)
+
+# Determine the Python site-packages directory
+execute_process(
+    COMMAND ${Python3_EXECUTABLE} -c "import site; print(site.getsitepackages()[0])"
+    OUTPUT_VARIABLE SITE_PACKAGES_DIR
+    OUTPUT_STRIP_TRAILING_WHITESPACE
+)
+message(STATUS "Python site-packages directory: ${SITE_PACKAGES_DIR}")
+
+#find_package(Torch REQUIRED)
+
+# Query PyTorchâ€™s include and library paths (as done in setup.py)
+execute_process(
+  COMMAND python3 -c "from torch.utils import cpp_extension; print(cpp_extension.include_paths()[0])"
+  OUTPUT_VARIABLE TORCH_INCLUDE_DIR
+  OUTPUT_STRIP_TRAILING_WHITESPACE
+)
+
+execute_process(
+  COMMAND python3 -c "from torch.utils import cpp_extension; print(cpp_extension.library_paths()[0])"
+  OUTPUT_VARIABLE TORCH_LIBRARY_DIR
+  OUTPUT_STRIP_TRAILING_WHITESPACE
+)
+
+execute_process(
+  COMMAND python3 -c "from torch.utils import cpp_extension; print(cpp_extension.include_paths()[1])"
+  OUTPUT_VARIABLE TORCH_API_INCLUDE_DIR
+  OUTPUT_STRIP_TRAILING_WHITESPACE
+)
+
+execute_process(
+  COMMAND python3 -c "import torch; import glob; print(';'.join(glob.glob(torch.__path__[0] + '/lib/*.so')))"
+  OUTPUT_VARIABLE TORCH_LIBRARIES
+  OUTPUT_STRIP_TRAILING_WHITESPACE
+)
+
+message(STATUS "PyTorch include directory: ${TORCH_INCLUDE_DIR}")
+message(STATUS "PyTorch library directory: ${TORCH_LIBRARY_DIR}")
+message(STATUS "PyTorch API include directory: ${TORCH_API_INCLUDE_DIR}")
+message(STATUS "PyTorch libraries: ${TORCH_LIBRARIES}")
+
+
+execute_process(
+  COMMAND python3 -c "import torch; print(int(torch._C._GLIBCXX_USE_CXX11_ABI))"
+  OUTPUT_VARIABLE GLIBCXX_USE_CXX11_ABI
+  OUTPUT_STRIP_TRAILING_WHITESPACE
+)
+message(STATUS "Detected _GLIBCXX_USE_CXX11_ABI=${GLIBCXX_USE_CXX11_ABI}")
+add_definitions(-D_GLIBCXX_USE_CXX11_ABI=${GLIBCXX_USE_CXX11_ABI})
+
+
+# Include directories for Python, PyTorch, and CUDA
+include_directories(${Python3_INCLUDE_DIRS})
+include_directories(${TORCH_INCLUDE_DIR})
+include_directories(${TORCH_API_INCLUDE_DIR})
+#include_directories(${Torch_INCLUDE_DIRS})
+include_directories(${CUDA_INCLUDE_DIRS})
+
+include_directories(${CMAKE_CURRENT_SOURCE_DIR}/lietorch/include)
+include_directories(${CMAKE_CURRENT_SOURCE_DIR}/eigen)
+
+# Add PyTorch and Python library directories
+link_directories(${CUDA_LIBRARY_DIRS})
+link_directories(${TORCH_LIBRARY_DIR})
+
+# Set up extra compile arguments (here we simply append -std=c++17 and -O2)
+# You might add more flags (e.g., -allow-unsupported-compiler) as needed.
+list(APPEND CUDA_NVCC_FLAGS "-std=c++${CMAKE_CXX_STANDARD}")
+list(APPEND CUDA_NVCC_FLAGS "-O2")
+set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr")
+
+# Build the shared libraries for the two extension modules.
+# Note: We remove the default "lib" prefix so that the modules are named 
+# exactly as Python expects (e.g. "lietorch_backends.so")
+add_library(lietorch_backends SHARED
+  lietorch/src/lietorch_gpu.cu
+  lietorch/src/lietorch.cpp
+  lietorch/src/lietorch_cpu.cpp
+)
+set_target_properties(lietorch_backends PROPERTIES PREFIX "")
+
+add_library(lietorch_extras SHARED
+  lietorch/extras/altcorr_kernel.cu
+  lietorch/extras/corr_index_kernel.cu
+  lietorch/extras/se3_builder.cu
+  lietorch/extras/se3_inplace_builder.cu
+  lietorch/extras/se3_solver.cu
+  lietorch/extras/extras.cpp
+)
+set_target_properties(lietorch_extras PROPERTIES PREFIX "")
+
+# Apply compile options to both targets
+target_compile_options(lietorch_backends PUBLIC ${CUDA_FLAGS} ${CUDA_NVCC_FLAGS})
+target_compile_options(lietorch_extras PUBLIC ${CUDA_FLAGS} ${CUDA_NVCC_FLAGS})
+
+# Link with PyTorch, Python, and CUDA libraries
+target_link_libraries(lietorch_backends PUBLIC ${TORCH_LIBRARIES} ${Python3_LIBRARIES} ${CUDA_LIBRARIES})
+target_link_libraries(lietorch_extras PUBLIC ${TORCH_LIBRARIES} ${Python3_LIBRARIES} ${CUDA_LIBRARIES})
+
+# Specify output directories (this is optional and can be adjusted)
+set_target_properties(lietorch_backends PROPERTIES LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)
+set_target_properties(lietorch_extras PROPERTIES LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)
+
+# Define the TORCH_EXTENSION_NAME for each module
+target_compile_definitions(lietorch_backends PRIVATE TORCH_EXTENSION_NAME=lietorch_backends)
+target_compile_definitions(lietorch_extras PRIVATE TORCH_EXTENSION_NAME=lietorch_extras)
+
+# Install the built extension modules into a package directory.
+# We also install the entire "lietorch" folder (which should include __init__.py and other Python files).
+install(TARGETS lietorch_backends DESTINATION ${SITE_PACKAGES_DIR})
+install(TARGETS lietorch_extras DESTINATION ${SITE_PACKAGES_DIR})
+install(DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/lietorch/ DESTINATION ${SITE_PACKAGES_DIR}/lietorch)
+
+
+message(STATUS "Build type: ${CMAKE_BUILD_TYPE}")
+message(STATUS "CMAKE_CXX_STANDARD: ${CMAKE_CXX_STANDARD}")
+message(STATUS "Output directory: ${CMAKE_BINARY_DIR}")
+
+# Display GCC build flags
+message(STATUS "GCC CXX flags: ${CMAKE_CXX_FLAGS}")
+
+# If CUDA is used, display NVCC flags
+if(CMAKE_CUDA_COMPILER)
+    message(STATUS "NVCC flags: ${CMAKE_CUDA_FLAGS} ${CUDA_FLAGS} ${CUDA_NVCC_FLAGS}")
+endif()
+
diff --git a/build.sh b/build.sh
new file mode 100755
index 0000000..f027ba1
--- /dev/null
+++ b/build.sh
@@ -0,0 +1,36 @@
+#!/bin/bash
+
+SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd ) # get script dir (this should be the main folder directory of PLVS)
+SCRIPT_DIR=$(readlink -f "$SCRIPT_DIR")  # this reads the actual path if a symbolic directory is used
+
+cd $SCRIPT_DIR
+
+# instead of building with setup.py use cmake and ninja
+# pip install . --verbose
+
+if [ ! -d build ]; then
+    mkdir build
+fi
+cd build
+
+# get how many cores are available
+cores=$(grep -c ^processor /proc/cpuinfo)
+# use half of them 
+cores=$((cores/2))
+
+echo "Building with $cores cores"
+
+# use ninja if available 
+if command -v ninja >/dev/null 2>&1; then
+    cmake -G Ninja ..
+    # launch parallel build with 8 threads
+    ninja -j$cores
+    ninja install
+else 
+    echo "ninja not found, falling back to make"    
+    cmake ..
+    make -j$cores
+    sudo make install  
+fi
+
+cd $SCRIPT_DIR
\ No newline at end of file
diff --git a/setup.py b/setup.py
index 09c8650..0a11d64 100644
--- a/setup.py
+++ b/setup.py
@@ -1,11 +1,97 @@
 from setuptools import setup
 from torch.utils.cpp_extension import BuildExtension, CUDAExtension
 
+import subprocess
+import os
 import os.path as osp
 
 
+NUM_PARALLEL_BUILD_JOBS = 1 # It seems setting more than 1 job does not work here! There seems to a race condition at build time for some reason.
+
 ROOT = osp.dirname(osp.abspath(__file__))
 
+
+# Initialize gcc major version
+gcc_major_version = 0
+
+# Get the version of g++
+try: 
+    # Run the command to get the g++ version
+    result = subprocess.run(['g++', '--version'], capture_output=True, text=True)
+    if result.returncode == 0:
+        # Extract version from the output
+        version_line = result.stdout.splitlines()[0]
+        version = version_line.split()[-1]  # The last element is the version
+        print(f"g++ version: {version}")
+
+        # Check if the version supports C++20 (g++ 10 and above support it)
+        gcc_major_version = int(version.split('.')[0])
+        print(f"gcc major version: {gcc_major_version}")
+    else:
+        print("Failed to get g++ version")        
+except Exception as e:
+    print(f"Failed to get g++ version: {e}")
+    
+
+def get_supported_architectures():
+    # Use `nvcc --list-gpu-arch` to list supported architectures
+    try:
+        result = subprocess.run(["nvcc", "--list-gpu-arch"], capture_output=True, text=True)
+        if result.returncode == 0:
+            architectures = result.stdout.splitlines()
+            return [arch.split('_')[1] for arch in architectures if arch.startswith("compute_")]
+        else:
+            print("Could not retrieve architectures. Using defaults.")
+    except FileNotFoundError:
+        print("nvcc not found. Make sure CUDA is installed and in PATH.")
+    # Return a default list if nvcc is unavailable
+    return ["60", "61", "70", "75", "80", "86"]
+
+
+
+cxx_compiler_flags = ['-O2']
+
+if os.name == 'nt':
+    cxx_compiler_flags.append("/wd4624")
+
+# Check nvcc version and set the appropriate flags.
+# Make sure that the nvcc executable is available in $PATH variables,
+# or find one according to the $CUDA_HOME variable
+try:
+    nvcc_std = subprocess.run("nvcc -h | grep -- '--std'", shell=True, capture_output=True, text=True)
+    nvcc_std_output = nvcc_std.stdout
+    
+    nvcc_flags = ['-O2', '-allow-unsupported-compiler']
+    if 'c++20' in nvcc_std_output and gcc_major_version >= 10:
+        nvcc_flags.append('-std=c++20')
+        cxx_compiler_flags.append('-std=c++20')
+    elif 'c++17' in nvcc_std_output:
+        nvcc_flags.append('-std=c++17')
+        cxx_compiler_flags.append('-std=c++17')
+    elif 'c++14' in nvcc_std_output:
+        nvcc_flags.append('-std=c++14')
+        cxx_compiler_flags.append('-std=c++14')
+except Exception as e:
+    print(f"Failed to get nvcc version: {e}")
+    nvcc_flags = ['-O2', '-allow-unsupported-compiler']  # Default flags if nvcc check fails
+    
+supported_architectures = get_supported_architectures()
+for arch in supported_architectures:
+    nvcc_flags.append(f"-gencode=arch=compute_{arch},code=sm_{arch}")
+
+    
+print(f"nvcc flags: {nvcc_flags}")
+print(f"cxx flags: {cxx_compiler_flags}")
+    
+
+class CustomBuildExtension(BuildExtension):
+    def build_extensions(self):
+        # Enable parallel builds
+        self.parallel = NUM_PARALLEL_BUILD_JOBS
+        print(f"Building with {self.parallel} parallel jobs")  # Debug message        
+        super().build_extensions()
+        
+
 setup(
     name='lietorch',
     version='0.2',
@@ -18,19 +104,12 @@ setup(
                 osp.join(ROOT, 'lietorch/include'), 
                 osp.join(ROOT, 'eigen')],
             sources=[
+                'lietorch/src/lietorch_gpu.cu',                
                 'lietorch/src/lietorch.cpp', 
-                'lietorch/src/lietorch_gpu.cu',
                 'lietorch/src/lietorch_cpu.cpp'],
             extra_compile_args={
-                'cxx': ['-O2'], 
-                'nvcc': ['-O2',
-                    '-gencode=arch=compute_60,code=sm_60', 
-                    '-gencode=arch=compute_61,code=sm_61', 
-                    '-gencode=arch=compute_70,code=sm_70', 
-                    '-gencode=arch=compute_75,code=sm_75',
-                    '-gencode=arch=compute_75,code=compute_75',
-                    
-                ]
+                'cxx': cxx_compiler_flags, 
+                'nvcc': nvcc_flags
             }),
 
         CUDAExtension('lietorch_extras', 
@@ -43,18 +122,11 @@ setup(
                 'lietorch/extras/extras.cpp',
             ],
             extra_compile_args={
-                'cxx': ['-O2'], 
-                'nvcc': ['-O2',
-                    '-gencode=arch=compute_60,code=sm_60', 
-                    '-gencode=arch=compute_61,code=sm_61', 
-                    '-gencode=arch=compute_70,code=sm_70', 
-                    '-gencode=arch=compute_75,code=sm_75',
-                    '-gencode=arch=compute_75,code=compute_75',
-                    
-                ]
+                'cxx': cxx_compiler_flags, 
+                'nvcc': nvcc_flags
             }),
     ],
-    cmdclass={ 'build_ext': BuildExtension }
+    cmdclass={ 'build_ext': CustomBuildExtension }
 )
 
 
